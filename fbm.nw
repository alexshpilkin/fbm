\documentclass{scrartcl}
\usepackage{selinput}
\SelectInputMappings{eacute={é},cyrf={ф}}
\usepackage[T2A,T1]{fontenc}
\usepackage[russian,english]{babel}
\usepackage[final,activate={true,nocompatibility}]{microtype}
\usepackage[tbtags]{amsmath}
\usepackage[shortlabels]{enumitem}
\usepackage{mathtools}
\usepackage{hyperref}

\renewcommand\rm{\normalfont\rmfamily}
\renewcommand\bf{\normalfont\bfseries}
\renewcommand\it{\normalfont\itshape}
\renewcommand\tt{\normalfont\ttfamily}
\usepackage{noweb}
\noweboptions{breakcode}

\makeatletter\@removefromreset{paragraph}{subsubsection}\makeatother
\renewcommand\theparagraph{\arabic{paragraph}}
\setkomafont{paragraph}{\sffamily\bfseries}
\newlength\paragraphskip \setlength\paragraphskip{.6ex plus.4ex minus.3ex}
\newlength\paragraphindent \setlength\paragraphindent{0pt}
\newlength\paragraphsep \setlength\paragraphsep{.8em}
\renewcommand\paragraph{%
  \par\addpenalty{-500}\addvspace\paragraphskip
  {\parindent=\paragraphindent\leavevmode}\refstepcounter{paragraph}%
  {\normalfont\usekomafont{paragraph}\paragraphformat}%
  \hskip\paragraphsep\ignorespaces}
\renewcommand\paragraphformat{\theparagraph.}

\let\term=\textit
\let\conj=\overline
\renewcommand\Re{\operatorname{Re}}
\renewcommand\Im{\operatorname{Im}}
\DeclareMathOperator*\res{res}
\let\eye=i  \let\pie=\pi
\renewcommand\cite[1]{[#1]}  % FIXME real citations

\title{Fractional Brownian motion}
\makeatletter\hypersetup{pdftitle=\@title,pdfauthor=\@author}\makeatother
\begin{document}

\paragraph Consider the motion of a particle---or, indeed, the evolution of a variable---that is continuously affected by many small momentary forces due to its interaction with the environment.  It is natural to represent the position of this particle by a stochastic process~$(X(t))$ with continuous sample paths, and we can assume $X(0) = 0$ for convenience.  If the forces at different moments in time are independent, the process~$(X(t))$ should have independent increments, that is any collection of increments $X(t_i)-X(s_i)$ should be independent as long as the intervals~$(s_i,t_i)$ do not intersect.  This means that the position~$X(t)$ is a sum of many independent random variables~$X(kt/n)-X((k-1)t/n)$, $1\leq k\leq n$.  It turns out that this property together with the requirement of continuity is enough to apply the Lindeberg central limit theorem and prove that $X(t)$ is Gaussian at every time~$t$ \cite{\foreignlanguage{russian}{\textit{Гихман, Скороход}}, 6\,§3}.  From there it is not difficult to see that all joint distributions of $(X(t_1),\ldots, X(t_n))$ must be Gaussian as well; the process~$(X(t))$ itself is consequently called Gaussian.
% FIXME "not difficult" --- how?

We have derived Gaussianity from the seemingly benign requirements of continuity of paths and independence of increments.  Let us now assume that the properties of the environment do not change with time, so the process $(X(t))$ has stationary increments, that is the joint distribution of any collection of increments $X(t_i+u)-X(s_i+u)$ does not depend on $u$.  Then both the mean~$\langle X(t)\rangle$ and the variance~$\langle(X(t)-\langle X(t)\rangle)^2\rangle$ grow linearly with time, and after subtracting a linear drift term~$\mu t$, $\mu =\langle X(1)\rangle$, we have a linearly growing mean squared displacement $\langle X(t)^2\rangle = 2Dt$, $2D =\langle X(1)^2\rangle$.  This behaviour is so common that we say our particle exhibits \term{normal diffusion}.

It turns out that the properties of Gaussianity, stationary increments, zero drift and normal diffusion determine our process completely.  To see this, observe that any centred Gaussian distribution, including the joint distribution of $(X(t_1),\ldots, X(t_n))$, is completely determined by its covariance matrix, in our case by the covariances $\langle X(t_i)X(t_j)\rangle$.  These can be computed by polarization:
\begin{equation}\label{polarization}
\begin{split}
  \langle X(t)X(s)\rangle
  &{}= (-1/2)\,\langle [X(t)-X(s)]^2 - X(t)^2 - X(s)^2\rangle \\
  &{}= (-1/2)\,(2D\lvert t-s\rvert - 2Dt - 2Ds) \\
  &{}= 2D\min\{s, t\}.
\end{split}
\end{equation}
The unique process we have thus picked out is called \term{Brownian motion} with diffusion constant~$D$.  It is ubiquitous in nature, from physics to economics, and for good reason:  as we have just shown, it is characterized by the very mild conditions of continuity of paths and stationarity and independence of increments \cite{\textit{Knight} 1}.  For example, every discrete-time random walk on discrete or continuous space has Brownian motion as its continuous-time limit as long as that continuous-time limit has continuous paths.  Normal diffusion is also commonplace exactly because it is exhibited by Brownian particles.  % FIXME other characterizations?

\paragraph As omnipresent as Brownian motion is, it is not enough to describe all instances of continuous-time random motion in nature.  Non-Brownian behaviour most commonly manifests itself as diffusion that does not obey the law $\langle X(t)^2\rangle\propto t$, so the name \term{anomalous diffusion} has come to denote the whole field of inquiry into random behaviour beyond the Brownian regime.  If we are to describe such phenomena, however, we will have to give up one or more of the properties discussed above.  This gives us three main directions of generalization \cite{\textit{Delorme, Wiese}}:
\begin{enumerate}[(a)]
  \item\emph{discontinuous} paths: the process is a continuous-time analogue of a random walk, but the increments have non-Gaussian infinitely divisible distributions;  % FIXME continuous-time limits of Lévy flights and CTRW with heavy-tailed increments; Lévy-Khintchine classification
  \item\emph{non-stationary} increments: the process describes Gaussian diffusion, but the diffusion coefficient is time-dependent;  % FIXME continuous-time limit of CTRW with infinite-mean waiting times [Meerschaert, Scheffler; Гребеньков] -- non-Gaussian?
  \item\emph{non-independent} increments: the process may be Gaussian or non-Gaussian, and the increments are correlated.
\end{enumerate}
Fractional Brownian motion~\cite{\foreignlanguage{russian}{\textit{Колмогоров}}; \textit{Mandelbrot, Van Ness}} with Hurst parameter~$H$ is a Gaussian process in the last family, the only centred one that conforms to the power law for anomalous diffusion $\langle X(t)^2\rangle\propto t^{2H}$.  This is easy to see: for any process~$(X(t))$ with $\langle X(t)^2\rangle = 2Dt^{2H}$, we can compute the covariances in a manner similar to Eq.~\eqref{polarization},
\begin{equation*}
\begin{split}
  \langle X(t)X(s)\rangle
  &{}= (-1/2)\,\langle [X(t)-X(s)]^2 - X(t)^2 - X(s)^2\rangle \\
  &{}= (-1/2)\,(2D\lvert t-s\rvert^{2H} - 2Dt^{2H} - 2Ds^{2H}) \\
  &{}= D(s^{2H} + t^{2H} -\lvert t-s\rvert^{2H})\,,
\end{split}
\end{equation*}
and a centred Gaussian process is determined uniquely by them;  in particular, Brownian motion is recovered for~$H = 1/2$, and \term{ballistic motion} $(X(t)) = (Vt)$ with $V$ a Gaussian random variable for~$H = 1$.  We will need this covariance matrix later for the purposes of sampling, so let us transcribe the formula above into code.
% FIXME delete the assertions?
<<Compute value covariances at [[times[]]] and [[time]]>>=
assert(time >= 0);
for (size_t i = 0; i < size; i++) {
	assert(times[i] >= 0);
	cov[i] = powr(times[i], 2*hurst) + powr(time, 2*hurst) -
	         powr(fabsr(times[i] - time), 2*hurst);
}
@ Here the value of $H$ is stored in the variable [[hurst]] and $D$ is set to unity.  The preprocessor macro~[[powr]] expands to one of [[pow]], [[powl]], or [[powq]] depending on which floating-point type is used as [[real_t]] in the program.  Other mathematical library functions such as [[fabs]] and [[sqrt]] are also wrapped in this manner.  % FIXME  what is real_t? "working precision"

% FIXME use $\Delta t$ not $h$ in the following? the code calls it [[pbdt]].
The variables~$Y_k = X(kh) - X((k-1)h)$ constructed from fractional Brownian motion~$(X(t))$ with Hurst parameter~$H$ form a stationary Gaussian sequence known as \term{fractional Gaussian noise} with parameter~$H$.  We have for the mean $\langle Y_k\rangle =\langle Y_1\rangle =\langle X(h)\rangle = 0$ and for the covariance
\begin{equation*}
\begin{split}
  \langle Y_{k+\ell}Y_k\rangle
  &{}=\langle Y_{\ell+1}Y_1\rangle \\
  &{}=\langle[X((\ell + 1)h) - X(\ell h)]X(h)\rangle \\
  &{}= Dh^{2H}[(\ell + 1)^{2H} - 2\ell^{2H} + (\ell - 1)^{2H}]\,,
\end{split}
\end{equation*}
% FIXME discuss positive and negative correlation for H < and > 1/2?
which again completely determines the sequence.  Stationary sequences turn out to be easier to sample than stationary-increment ones, so several approaches to sampling fractional Brownian motion on a uniform grid instead sample fractional Gaussian noise and sum the result.  We will use such an approach when we need to sample fractional Brownian motion on a large uniform grid.
<<Compute noise covariances>>=
real_t expmult = powr(pbdt, 2*hurst),
       prevexp, currexp = expmult, nextexp = 0.0;
for (size_t i = 0; i < pbsize; i++) {
	prevexp = currexp; currexp = nextexp;
	nextexp = powr(i+1, 2*hurst);
	eigen[i] = expmult*(nextexp - 2.0*prevexp + currexp);
}
<<Compute times and sum noise to values>>=
real_t sum = 0.0;
for (size_t i = 0; i < pbsize; i++) {
	pbtimes[i]  = (i + 1)*pbdt;
	pbvalues[i] = sum += noise[i];
}
@ Here and in the sequel, the value $X(0) = 0$ is not stored.

\paragraph Consider the action of space and time rescalings on a stochastic process~$(X(t))$ starting at zero.  If one can always be counteracted by the other, that is if for any time scaling factor~$a$ there is a space scaling factor~$b(a)$ such that $(X(at)) = (b(a)X(t))$, and $b$ is not identically equal to one, the process~$(X(t))$ is called \term{self-similar}.  Observe that $b(a'a'') = b(a')b(a'')$; under mild technical conditions, we also have that $b(a)$ is continuous, so $b(a) = a^H$ for a certain~$H$~\cite{\textit{Lamperti}; \textit{Embrechts, Maejima} 1.1}.  The process~$(X(t))$ is then more specifically called \term{$H$-self-similar} and $H$ is called its \term{Hurst parameter}.  Every $H$-self-similar process~$(X(t))$ starts at zero:  indeed, $a^H X(0)$ is distributed identically for all~$a$, and we can set $a\to 0$.  Together with stationarity of increments, $H$-self-similarity for $H\neq 1$ implies zero means \cite{\textit{Taqqu}, see \textit{Rao}}: $2^H\langle X(t)\rangle =\langle X(2t)\rangle =\langle [X(2t)-X(t)] + [X(t)-X(0)]\rangle = 2\langle X(t)\rangle$.

In the case of fractional Brownian motion, we have
\begin{equation*}
\begin{split}
  \langle X(at)X(as)\rangle
  &{}= D[(as)^{2H} + (at)^{2H} -\lvert a(t-s)\rvert^{2H}] \\
  &{}= a^{2H}D(s^{2H} + t^{2H} -\lvert t-s\rvert^{2H}) \\
  &{}=\langle a^H X(t)\times a^H X(t)\rangle
\end{split}
\end{equation*}
and $\langle X(at)\rangle =\langle a^H X(t)\rangle = 0$, so by Gaussianity $(X(at)) = (a^H X(t))$.  Fractional Brownian motion with Hurst parameter~$H$ is therefore $H$-self-similar, justifying the dual use of the term \textit{Hurst parameter}.  Moreover, for $H\neq 1$ it is the only $H$-self-similar Gaussian process with stationary increments, because $H$-self-similarity and stationary increments imply power-law anomalous diffusion $\langle X(t)^2\rangle = t^{2H}\langle X(1)^2\rangle\propto t^{2H}$ as well as zero means and starting at zero.
% This self-similarity is also apparent in the dimension the diffusion constant~$D$.

The self-similarity properties of fractional Brownian motion can also be expressed in terms of fractional Gaussian noise~\cite{\foreignlanguage{russian}{\textit{Синай}}; \textit{Embrechts, Maejima} 2.2; \textit{Samorodnitsky, Taqqu} 7.2.3}.  For a stationary sequence $(Y_k)$, define the \term{renormalization group} transformation of index~$H$,
\begin{equation*}
  (Y_k)\mapsto (A_n Y_k),\quad A_n Y_k =\frac 1{n^H}\sum_{\ell = n(k-1)+1}^{nk} Y_\ell\,.
\end{equation*}
If the variables~$Y_k = X(kh) - X((k-1)h)$ are constructed from an $H$-self-similar process~$(X(t))$ with stationary increments, then $(Y_k)$ will be fixed by the renormalization group:  the renormalized variables
\begin{equation*}
\begin{split}
  A_n Y_k
  &{}=\frac 1{n^H}\sum_{\ell = n(k-1)+1}^{nk}[X(\ell h) - X((\ell-1)h)] \\
  &{}=\frac 1{n^H}[X(nkh) - X(n(k-1)h)]
\end{split}
\end{equation*}
are distributed identically to the original variables $Y_k\,$.  To go in the other direction, from a fixed point of the renormalization group of index $H$ to an $H$-self-similar process, set
\begin{equation*}
  X(t) =\lim_{n\to\infty} \frac 1{n^H}\sum_{k = 1}^{\lfloor nt/h\rfloor} Y_k\,.
\end{equation*}
This correspondence together with the characterization of fractional Brownian motion above implies that fractional Gaussian noise with parameter $H$ is the unique Gaussian fixed point of the renormalization group of index $H\neq 1$.

\paragraph We have so far focused on unique characterizations, but glossed over existence questions;  let us now discuss those in an informal manner and simultaneously establish a link with the original description of fractional Brownian motion by Mandelbrot and Van~Ness~\cite{\textit{Mandelbrot, Van~Ness}}.

First of all, if fractional Brownian motion~$(X(t))$ with Hurst parameter $H$ is to exist, the covariance matrix
\begin{equation*}
  C_{ij} =\langle X(t_i)X(t_j)\rangle = D(t_i^{2H} + t_j^{2H} + \lvert t_i-t_j\rvert^{2H})
\end{equation*}
must be positive semidefinite for any sequence of times $(t_i)$.  However, for $H > 1$, setting $(t_i) = (1, 2)$ and $(a_i) = (-2, 1)$ gives $\sum a_i C_{ij} a_j = 2D(4 - 2^{2H}) < 0$, so fractional Brownian motion does not exist.  For $0 < H\leq 1$, one can in fact prove that $(C_{ij})$ is always positive definite and use a general theorem to establish the existence of fractional Brownian motion~\cite{\textit{Nourdin} 1.5}, but we will follow a more explicit approach.

Ordinary Brownian motion~$(B(t))$ can be constructed as a continuous-time limit of random walks \cite{\textit{Knight} 1.3}.  Its derivative requires more work \cite{\foreignlanguage{russian}{\textit{Гельфанд, Виленкин}}; \textit{Hida}; \textit{Øksendal}; \textit{Oliveira}}, but at least informally it is a centred stationary Gaussian process~$(\dot B(t))$ with covariance $\langle\dot B(t)\dot B(s)\rangle = 2D\,\delta(t-s)$.  The derivative is $(-1/2)$-self-similar;  as expected, differentiation lowers the Hurst parameter by one and integration raises it by one.  This suggests that an $H$-self-similar process can be constructed by integrating $(\dot B(t))$ from zero a nonintegral number $H + 1/2$ of times.  It is not apparent why this \term{Riemann--Liouville process} should have stationary increments---and in fact it does not, so it is different from fractional Brownian motion \cite{\textit{Marinucci, Robinson}; \textit{Lifshits} 7.3.4}.  The $(H-1)$-self-similar process~$(\dot X(t))$ obtained by integrating $(\dot B(t))$ from negative infinity $H - 1/2$ times should be stationary, however, as long as our hypothetical fractional integral is translation invariant in the sense that integrating $f({\cdot}-b)$ from~$-\infty$ to~$a+b$ is the same as integrating $f$ from~$-\infty$ to~$a$.  Fractional Brownian motion is then obtained by an ordinary indefinite integral of $(\dot X(t))$ from zero.  To implement this program, we have to first establish a theory of integration of fractional order, so let us get to that now.

% FIXME "white noise" ?
% FIXME RLp is also sometimes caled fBm

\paragraph Indefinite integration
\begin{equation*}
  I_+ f(s) =\int_{-\infty}^s dt\,f(t) =\int dt\,\theta(s-t)f(t) =\theta\ast f(s)
\end{equation*}
can be expressed as convolution with kernel $\Phi_1(s) =\theta(s)$ equal to the Heaviside function.  So it is no surprise that repeated indefinite integrations can be expressed as convolutions as well:
\begin{align*}
  I_+^2 f(s) &{}=\begin{aligned}[t]
    \Phi_1\ast\Phi_1\ast f(s)
    &{}=\Phi_2\ast f(s),\\
    &\phantom{{}={}}\Phi_2(s) =\int dt\,\theta(s-t)\,\theta(t) =\frac{s\,\theta(s)}1\,,
  \end{aligned}\\
  I_+^3 f(s) &{}=\begin{aligned}[t]
    \Phi_1\ast\Phi_2\ast f(s)
    &{}=\Phi_3\ast f(s),\\
    &\phantom{{}={}}\Phi_3(s) =\int dt\,\theta(s-t)\,\frac{t\theta(t)}1 =\frac{s^2\theta(s)}{1\times 2}\,,
  \end{aligned}\\
  &{}\vdotswithin{=}\\
  I_+^{k+1}f(s) &{}=\begin{aligned}[t]
    \Phi_1\ast\Phi_k\ast f(s)
    &{}=\Phi_{k+1}\ast f(s),\\
    &\phantom{{}={}}\Phi_{k+1}(s) =\int dt\,\theta(s-t)\,\frac{t^{k-1}\theta(t)}{(k-1)!} =\frac{s^k\theta(s)}{k!}\,.
  \end{aligned}
\end{align*}
This is the \term{Cauchy formula} for repeated integration.  The fact that $k$ is an integer is immaterial to the final expression:  we can set for any complex $\alpha$ with $\Re\alpha > 0$
\begin{equation*}
  I_+^\alpha f(s) =\Phi_\alpha\ast f(s),\quad\Phi_\alpha(s) =\frac{s^{\alpha-1}\theta(s)}{\Gamma(\alpha)}\,.
\end{equation*}
The resulting operation is called the Riemann--Liouville \term{fractional integral} on the real line~\cite{\textit{Oldham, Spanier}; \textit{Miller, Ross}; \textit{Podlubny}; \textit{Samko, Kilbas, Marichev}; \foreignlanguage{russian}{\textit{Гельфанд, Шилов}~I.5.5}}.  A short calculation shows that $\Phi_\alpha\ast\Phi_\beta =\Phi_{\alpha +\beta}$ and so the semigroup property $I_+^{\vphantom\beta\alpha} I_+^\beta = I_+^{\alpha +\beta}$ implied by the notation is valid.  The convolution theorem for the Fourier transform gives $I_+^\alpha f(\omega) =\Phi_\alpha(\omega)f(\omega)$ with $\Phi_\alpha(\omega) = \exp[\eye\pie\alpha/2](\omega +\eye 0)^{-\alpha}$ \cite{\foreignlanguage{russian}{\textit{Гельфанд, Шилов}~II.2.3}}, or schematically $I_+^\alpha f(\omega) = (-\eye\omega)^{-\alpha} f(\omega)$, as expected for the Fourier transform of an integral of order~$\alpha$.  Finally, $\Phi_\alpha$ is homogeneous of degree~$\alpha - 1$, so if $f$ is homogeneous of degree $H$, then $I_+^\alpha f$ is homogeneous of degree~$H +\alpha$ due to the factor $\Phi_\alpha(s)\,ds$ in the convolution integral, again as expected for an integral of order~$\alpha$.

The restriction $\Re\alpha > 0$ for the order of integration is, of course, unsatisfactory;  we would at the very least like to have $I_+^0 f(s) = f(s)$, $\Phi_0(s) =\delta(s)$.  A theory that allows arbitrary orders of integration~$\alpha$ and includes differentiation at~$\alpha = -1, -2,\ldots$ can be developed and is described in the references, but for our purposes it will be sufficient to extend the domain of definition to $\Re\alpha > -1$.  One proceeds by analytic continuation~\cite{\foreignlanguage{russian}{\textit{Гельфанд, Шилов}~I.3.2, I.3.5}}, noting that $\int \Phi_\alpha(t)\phi(t)dt$ is holomorphic in~$\alpha$ for any test function~$\phi$ and that the right-hand side of the equality
\begin{equation*}
  \int\Phi_\alpha(t)\phi(t)dt =\frac 1{\Gamma(\alpha)}\left\{
    \int_0^a t^{\alpha-1}[\phi(t) -\phi(0)]dt +
    \int_a^{+\infty} t^{\alpha-1}\phi(t)\,dt +
    \frac{a^\alpha\phi(0)}\alpha
  \right\}
\end{equation*}
% FIXME Hadamard’s parties finies [Гельфанд, Шилов; Samko, Kilbas, Marichev]
valid for any $a$ and $\Re\alpha > 0$ exists for $\Re\alpha > -1$, $\alpha\neq 0$, thus is the unique analytic continuation of the left-hand side for these values of $\alpha$.  The definition for the fractional derivative~$I_+^\alpha$, $-1 <\alpha < 0$, obtained by taking $a\to +\infty$ in this expression is known as the \term{Marchaud fractional derivative}~\cite{\textit{Samko, Kilbas, Marichev}; \textit{Pipiras, Taqqu}}.  Finally, we can confirm that $\Phi_\alpha(s) =\delta(s)$ at $\alpha = 0$, where both $\Gamma(\alpha)$ and the expression in braces have a pole, by setting $a = 1$ and taking residues:
\begin{equation*}
  \int\Phi_0(t)\phi(t)dt =
  \frac{\res_{\alpha = 0} \phi(0)/\alpha}{\res_{\alpha = 0}\Gamma(\alpha+1)/\alpha} =
  \phi(0) =\int\delta(t)\phi(t)dt.
\end{equation*}
We have therefore extended $I_+^\alpha$ to all $\alpha$ with $\Re\alpha > -1$.  By uniqueness of analytic continuation, the semigroup property $I_+^{\vphantom\beta\alpha} I_+^\beta = I_+^{\alpha +\beta}$ is valid whenever both sides of the equation are defined.  That $\Phi_\alpha$ is homogeneous of degree $\alpha - 1$ also follows from uniqueness.

We have defined the fractional integral with a variable upper limit.  We do not need to work out the theory for a variable lower limit because the corresponding integration operator of order one, $I_-$, is adjoint to~$I_+$:
\begin{multline*}
  \int ds\,\conj{f(s)}\,I_+g(s) =\int ds\,\conj{f(s)}\left(\int_{-\infty}^s dt\,g(t)\right) =\\
  =\int ds\,dt\,\conj{f(s)}\,\theta(s-t)\,g(t) =\\
  =\int dt\,\conj{\left(\int_t^{+\infty}ds\,f(s)\right)}\,g(t) =\int dt\,\conj{I_-f(t)}\,g(t).
\end{multline*}
We therefore set for any order~$\alpha$
\begin{equation*}
  I_-^\alpha f(s) =\Psi_\alpha\ast f(s),\quad\Psi_\alpha(s) =\Phi_\alpha(-s).
\end{equation*}
Fractional integration on a finite or semi-infinite interval can also be defined by extending the integrand to the whole real line by zero.

\paragraph Returning to the construction of fractional Brownian motion, we can now define~\cite{\textit{Pipiras, Taqqu} 3.1}
\begin{align*}
  \dot X(t) &= C I_+^{H-1/2}\dot B(t),\\
  X(t) &= C\int_0^t I_+^{H-1/2}\dot B(s) ds\\
  &= C\int\theta(s)\theta(t-s)I_+^{H-1/2}\dot B(s) ds\\
  &= C\int I_-^{H-1/2}[\theta(\cdot)\theta(t-\cdot)](s)\dot B(s) ds\\
  &=\frac C{\Gamma(H+1/2)}\int[(t-s)^{H-1/2}\theta(t-s) - (-s)^{H-1/2}\theta(-s)]\dot B(s) ds\\
  &=\frac C{\Gamma(H+1/2)}\left\{
    \int_{-\infty}^0 [(t-s)^{H-1/2} - (-s)^{H-1/2}]\,dB(s) +
    \int_0^t (t-s)^{H-1/2}\,dB(s)
  \right\}
\end{align*}
and obtain in the last line the definition of fractional Brownian motion used with $C = 1$ by Mandelbrot and Van Ness~\cite{\textit{Mandelbrot, Van Ness}}.  The requirement $\langle X(1)^2\rangle = 2D$ with $\langle B(1)^2\rangle = 1$ fixes $C$ to a different value~\cite{\textit{Picard} B}.

% FIXME FBm satisfies fractional Langevin equation

\end{document}
